-*- outline -*-

* Pending (before it can be used for ANYTHING)

* Pending before it can be used for single-user, multiple-installations

** Implement tree remote protocol

Essentially have to store locally from leaves onward; otherwise refcnts are
wrong. however, how does partial tree work in this case? it does not,
really.

so the remoting will have two modes:

- weak references ('cache') + remote access on demand

- strong references + stored remote trees to be merged
 - should store leaves first so we can correctly handle dependencies..

* Pending before it can be used (reliably) for multi-user

* Pending (eventually)

** Think through the timestamp updates

They should occur only at end of particular fuse operation, using same
fixed timestamp

** Write more thorough filesystem tests to test_ops or elsewhere

- 100% unit test coverage would be nice (for now, fstest does lots of heavy
  lifting but takes minute+ to complete)

** Fix fstest cases

given

# ( cd /tmp/x && prove5.18 -f -o -r ~mstenber/git/fstest/tests ) 2>&1 | tee log.txt ; umount /tmp/x

before this project started:

egrep '^not ok' ~/log.txt | wc -l

= 619

now: 85

Test Summary Report
-------------------
/Users/mstenber/git/fstest/tests/link/00.t    (Wstat: 0 Tests: 82 Failed: 44)
  Failed tests:  4-15, 18-20, 22, 24, 30-41, 44-46, 48, 50
                55-58, 60, 62-65, 67
/Users/mstenber/git/fstest/tests/link/02.t    (Wstat: 0 Tests: 10 Failed: 4)
  Failed tests:  2, 4-6
/Users/mstenber/git/fstest/tests/link/03.t    (Wstat: 0 Tests: 16 Failed: 5)
  Failed tests:  6, 8-11
/Users/mstenber/git/fstest/tests/link/06.t    (Wstat: 0 Tests: 18 Failed: 2)
  Failed tests:  7-8
/Users/mstenber/git/fstest/tests/link/07.t    (Wstat: 0 Tests: 17 Failed: 2)
  Failed tests:  7-8
/Users/mstenber/git/fstest/tests/link/09.t    (Wstat: 0 Tests: 5 Failed: 2)
  Failed tests:  2, 4

^ link cases not surprising, as tfhfs does not implement normal link
  semantics (yet?)

/Users/mstenber/git/fstest/tests/open/17.t    (Wstat: 0 Tests: 3 Failed: 1)
  Failed test:  2

^ some weirdness with mkfifo <> fuse (ENXIO <> EPERM)

/Users/mstenber/git/fstest/tests/rename/00.t  (Wstat: 0 Tests: 79 Failed: 16)
  Failed tests:  7-9, 11, 13-14, 27-29, 31, 33-34, 49, 53
                57, 61

^ early ones are link related; last ones are not, gnn; seems possibly OS X
  bug? still, just ctime wrong on rename, so not big problem, hopefully

/Users/mstenber/git/fstest/tests/unlink/00.t  (Wstat: 0 Tests: 55 Failed: 9)
  Failed tests:  15-17, 20-22, 51-53

^ again, link case

Files=184, Tests=1944, 77 wallclock secs ( 0.74 usr  0.38 sys + 12.05 cusr
10.10 csys = 23.27 CPU)

** Performance improvement ideas

*** empty file, no backend

dd if=/dev/null of=/tmp/x/foo bs=1 count=1 oseek=1000000000
dd if=/tmp/x/foo of=/dev/null bs=1024000
umount /tmp/x

=> 260 MB/s currently (with in-memory storage)

~1/3 of CPU time spent in Python, so I guess it is not really even Python
CPU bound but instead 2/3 of time spent in dd sets up the upper bound (at
least on OS X).

**** raw native hfs

file _creation_ (30GB file) = 68sec = <500 MB/s
file _reading_ (30GB file):
30000000000 bytes transferred in 30.563466 secs (981564065 bytes/sec) = 981MB/s

*** Rethink the parallelization scheme

.. for now, we do just parallel sha256, which seems to provide 'nice'
performance, but is it nice enough?

** Check/test

*** Insertion to various places (should work but who knows)
*** Reading in clever and unexpected ways (should work but who knows)

** Main (osx?) fuse improvements

*** max_write does not seem to be honored

still getting 64kb blocks, which is rather awkward (2-3 writes per 128kb
block received).

*** inodes seem to be kept open 'long time'

.. mostly fixed code to scale with it, but it should not be a problem

** Consider if CBORPickler should also support e.g. IntEnum as argument

** Work out how sharing of storage backends works

The main question is when to rm blocks for real. If we have exclusive
access to a storage, can rm them if there is no tree (either root-based
forest or inode based forest) referring to them.

Persisting inode-based references (in-memory construct) seems like
undesirable design (causes more disk churn). Ways around it:

*** (Always) exclusive use of storage backend

No need to worry about other writers => no need to do the churn to actually
disk.

*** No cleanups when shared

With one main writer, can have it get exclusive (r+w) lock, and drop it
infrequently or on demand if there are other potential writers. While
multiple writers are present, no cleanups.

*** Shared cleanups

- post read-only notice

- wait for other writers to _increment_ their inodes' reference counts to
  disk (and switch to read-only mode)

- rm blocks based on update dreference counts

- remove read-only notice

Electing the cleaner to be the one with most in-memory references seems
sensible but possibly minor optimization.

** Real supplementary group support

*** (For now, we parse pwd/grp, which, while nice, is not correct)

** Rewrite in e.g. Go?

*** https://github.com/hanwen/go-fuse/ looks mature

Every op in own coroutine -> trivially parallelizable _and_ reasonably easy
to get correct as well. (synchronization may be painful)

reasonable performance (at least based on their own 2012 benchmarks);
should know how llfuse compares though..


*** I probably want to use fixed inodes in the next iteration

Hardlinks are used by some apps -> if want to use transparently _any_
unix apps, hard links are must -> instead of directory tree, inode tree
should be core construct.

*** Correct readdir semantics

mutation should not result in non-mutated files being omitted/returned
twice (this should be trivially true given we iterate automatically by
filename within the tree order?); current implementation is bit vague on
the topic..
