-*- outline -*-

* Pending (before it can be used for ANYTHING)

** Refactor the inode allocation to work like fd allocation - reusing numbers

(Same utility mixin superclass / contained object?)

** Think how to decrease size of node trees when they consume too much memory

- if whole subtree is non-dirty, we could in theory just drop it..

** Check/test

*** Truncation
*** Insertion to various places
*** Reading in clever and unexpected ways

** Add missing functionality to forest module (e.g. remove API?)

** Write more thorough filesystem tests to test_ops or elsewhere

** Correct readdir semantics

mutation should not result in non-mutated files being omitted/returned
twice (this should be trivially given we iterate automatically by filename
within the tree order?)

* Pending 2 (before it can be used for ANYTHING); TBD P2 in code

* Pending before it can be used for single-user, multiple-installations

** Implement tree remote protocol

essentially have to store locally from leaves onward; otherwise refcnts are
wrong. however, how does partial tree work in this case? it does not,
really.

so the remoting will have two modes:

- weak references ('cache') + remote access on demand

- strong references + stored remote trees to be merged
 - should store leaves first so we can correctly handle dependencies..

** Implement tree merge code (SHOULD be straightforward, but who knows)

- given 'last point in sync with X', compare current local tree + X's
  current remote tree top.


* Pending before it can be used (reliably) for multi-user

** Sticky bit

* Pending (eventually)

** Consider if CBORPickler should also support e.g. IntEnum as argument

** Work out how sharing of storage backends works

The main question is when to rm blocks for real. If we have exclusive
access to a storage, can rm them if there is no tree (either root-based
forest or inode based forest) referring to them.

Persisting inode-based references (in-memory construct) seems like
undesirable design (causes more disk churn). Ways around it:

*** (Always) exclusive use of storage backend

No need to worry about other writers => no need to do the churn to actually
disk.

*** No cleanups when shared

With one main writer, can have it get exclusive (r+w) lock, and drop it
infrequently or on demand if there are other potential writers. While
multiple writers are present, no cleanups.

*** Shared cleanups

- post read-only notice

- wait for other writers to _increment_ their inodes' reference counts to
  disk (and switch to read-only mode)

- rm blocks based on update dreference counts

- remove read-only notice

Electing the cleaner to be the one with most in-memory references seems
sensible but possibly minor optimization.

** Rethink if inodestore <> forest abstraction is good
