-*- outline -*-

* Pending (before it can be used for ANYTHING)

** Fix fstest cases

given

# ( cd /tmp/x && prove5.18 -f -o -r ~mstenber/git/fstest/tests ) 2>&1 | tee log.txt ; umount /tmp/x

egrep '^not ok' ~/log.txt | wc
     619    1857    6126


** Write more thorough filesystem tests to test_ops or elsewhere

** Correct readdir semantics

mutation should not result in non-mutated files being omitted/returned
twice (this should be trivially given we iterate automatically by filename
within the tree order?)

* Pending before it can be used for single-user, multiple-installations

** Implement tree remote protocol

essentially have to store locally from leaves onward; otherwise refcnts are
wrong. however, how does partial tree work in this case? it does not,
really.

so the remoting will have two modes:

- weak references ('cache') + remote access on demand

- strong references + stored remote trees to be merged
 - should store leaves first so we can correctly handle dependencies..

** Implement tree merge code (SHOULD be straightforward, but who knows)

*** Bare bones code IS there, but it needs unit tests

* Pending before it can be used (reliably) for multi-user

** Sticky bit

* Pending (eventually)

** Performance improvement ideas

*** Rethink the parallelization scheme

.. for now, we do just parallel sha256, which seems to provide 'nice'
performance, but is it nice enough?

** Check/test

*** Insertion to various places (should work but who knows)
*** Reading in clever and unexpected ways (should work but who knows)

** Main (osx?) fuse improvements

*** max_write does not seem to be honored

still getting 64kb blocks, which is rather awkward (2-3 writes per 128kb
block received).

*** inodes seem to be kept open 'long time'

.. mostly fixed code to scale with it, but it should not be a problem

** Consider if CBORPickler should also support e.g. IntEnum as argument

** Work out how sharing of storage backends works

The main question is when to rm blocks for real. If we have exclusive
access to a storage, can rm them if there is no tree (either root-based
forest or inode based forest) referring to them.

Persisting inode-based references (in-memory construct) seems like
undesirable design (causes more disk churn). Ways around it:

*** (Always) exclusive use of storage backend

No need to worry about other writers => no need to do the churn to actually
disk.

*** No cleanups when shared

With one main writer, can have it get exclusive (r+w) lock, and drop it
infrequently or on demand if there are other potential writers. While
multiple writers are present, no cleanups.

*** Shared cleanups

- post read-only notice

- wait for other writers to _increment_ their inodes' reference counts to
  disk (and switch to read-only mode)

- rm blocks based on update dreference counts

- remove read-only notice

Electing the cleaner to be the one with most in-memory references seems
sensible but possibly minor optimization.
